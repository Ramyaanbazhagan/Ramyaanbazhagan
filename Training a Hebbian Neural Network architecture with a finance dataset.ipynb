{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMt8wwAwm4m3IBneQifAobI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ramyaanbazhagan/Ramyaanbazhagan/blob/main/Training%20a%20Hebbian%20Neural%20Network%20architecture%20with%20a%20finance%20dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "D2yq7rgNyyJs",
        "outputId": "469c8b36-736c-4f4f-f403-63ace4f35d5a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "no such column: sentence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bb4c313eb23b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Initialize chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mchatbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleChatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bb4c313eb23b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, db_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_database\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bb4c313eb23b>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT sentence FROM knowledge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such column: sentence"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import sqlite3\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def fetch_wikipedia_content(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        return \"\\n\".join([para.get_text() for para in paragraphs])\n",
        "    return \"\"\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha])\n",
        "\n",
        "class SimpleChatbot:\n",
        "    def __init__(self, db_name=\"chatbot.db\"):\n",
        "        self.db_name = db_name\n",
        "        self.create_database()\n",
        "        self.load_data()\n",
        "\n",
        "    def create_database(self):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS knowledge (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                sentence TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def store_data(self, text):\n",
        "        sentences = [sent.text.strip() for sent in nlp(text).sents]\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.executemany(\"INSERT INTO knowledge (sentence) VALUES (?)\", [(s,) for s in sentences])\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def load_data(self):\n",
        "        conn = sqlite3.connect(self.db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT sentence FROM knowledge\")\n",
        "        self.sentences = [row[0] for row in cursor.fetchall()]\n",
        "        conn.close()\n",
        "\n",
        "        if self.sentences:\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.tfidf_matrix = self.vectorizer.fit_transform(self.sentences)\n",
        "        else:\n",
        "            self.vectorizer = None\n",
        "            self.tfidf_matrix = None\n",
        "\n",
        "    def get_response(self, query):\n",
        "        if not self.sentences:\n",
        "            return \"I don't have any information stored yet.\"\n",
        "\n",
        "        query_vec = self.vectorizer.transform([query])\n",
        "        similarity = cosine_similarity(query_vec, self.tfidf_matrix).flatten()\n",
        "        best_match_index = similarity.argmax()\n",
        "\n",
        "        if similarity[best_match_index] > 0.2:  # Increase threshold to filter irrelevant responses\n",
        "            return self.sentences[best_match_index].split('.')[0] + \".\"  # Return only the first sentence\n",
        "        else:\n",
        "            return \"I'm not sure about that.\"\n",
        "\n",
        "# Fetch Wikipedia content\n",
        "url = \"https://en.wikipedia.org/wiki/Animal\"\n",
        "document_text = fetch_wikipedia_content(url)\n",
        "\n",
        "# Initialize chatbot\n",
        "chatbot = SimpleChatbot()\n",
        "chatbot.store_data(document_text)\n",
        "chatbot.load_data()\n",
        "\n",
        "# User interaction\n",
        "while True:\n",
        "    user_input = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "    response = chatbot.get_response(user_input)\n",
        "    print(\"Chatbot:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sfhky-xfAhOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZD9f-JDOA9e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Step 1: Define activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Step 2: Define loss function\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Step 3: Implement Hebbian Neural Network class\n",
        "class HebbianNN:\n",
        "    def __init__(self, input_size, output_size, learning_rate=0.01):\n",
        "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = sigmoid(np.dot(X, self.weights))\n",
        "        return self.output\n",
        "\n",
        "    def hebbian_update(self):\n",
        "        self.weights += self.learning_rate * np.dot(self.input.T, self.output)\n",
        "\n",
        "    def train(self, X, y, epochs=100):\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self.forward(X)\n",
        "            loss = loss_function(y, y_pred)\n",
        "            self.hebbian_update()\n",
        "            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Step 4: Load and preprocess the historical stock prices of Apple (AAPL)\n",
        "data = yf.download('AAPL', start='2010-01-01', end='2022-02-26')\n",
        "X = data[['Open', 'High', 'Low', 'Volume']]\n",
        "y = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data using Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "\n",
        "# Step 5: Train the Hebbian Neural Network\n",
        "nn = HebbianNN(input_size=X_scaled.shape[1], output_size=y_scaled.shape[1], learning_rate=0.01)\n",
        "nn.train(X_scaled, y_scaled, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpyhTeE4Fsbc",
        "outputId": "de2251f0-3518-43de-e037-e7d42323caab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1387\n",
            "Epoch 2, Loss: 0.3866\n",
            "Epoch 3, Loss: 0.5950\n",
            "Epoch 4, Loss: 0.6561\n",
            "Epoch 5, Loss: 0.6729\n",
            "Epoch 6, Loss: 0.6780\n",
            "Epoch 7, Loss: 0.6796\n",
            "Epoch 8, Loss: 0.6802\n",
            "Epoch 9, Loss: 0.6804\n",
            "Epoch 10, Loss: 0.6805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "psqYUSMgE-wS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}